{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Model v1\n",
    "\n",
    "Model v1 is a ML model for the proposed UI which accepts \"any\" inputs.\n",
    "\n",
    "We fine-tune model-v0 and TAPEX with the custom dataset annotated in the previous notebook.\n",
    "We modify the tokenizer from the original one to support variable length and keyword arguments.\n",
    "The model performance is improved \n",
    "\n",
    "As a result, the exact match ratio of the model-v0 fine-tuning model gets to be ~64% for the test dataset ([Weights & Biases](https://wandb.ai/kwkty/vxnli/runs/3r21665c)).\n",
    "And tha TAPEX fine-tuning model is ~54% ([Weights & Biases](https://wandb.ai/kwkty/vxnli/runs/3grk8w92)).\n",
    "\n",
    "It's better to use the former model, however, we use the latter one intentionally in the user study.\n",
    "Because we want to clarify that the performance of this model doesn't depend on the dataset size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir: str = \"../data/\"\n",
    "push_model_to_huggingface_hub: bool = True\n",
    "report_to_wandb: bool = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import multiprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TapexTokenizer,\n",
    "    trainer_utils,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "DATA_DIR: Path = Path(data_dir)\n",
    "\n",
    "MODEL_NAME: str = \"vxnli-v1\"\n",
    "\n",
    "DATABASE_DIR: Path = DATA_DIR.joinpath(\"datasets/nvBench/database\")\n",
    "DATASET_DIR: Path = DATA_DIR.joinpath(f\"datasets/{MODEL_NAME}/\")\n",
    "DATASET_OUTPUT_DIR: Path = DATA_DIR.joinpath(f\"datasets/{MODEL_NAME}.hf/\")\n",
    "\n",
    "MODEL_OUTPUT_DIR: Path = DATA_DIR.joinpath(f\"models/{MODEL_NAME}/\")\n",
    "RESULT_OUTPUT_DIR: Path = DATA_DIR.joinpath(f\"results/{MODEL_NAME}/\")\n",
    "\n",
    "# Model Parameters\n",
    "\n",
    "BASE_MODEL: str = \"microsoft/tapex-base-finetuned-wtq\"\n",
    "# BASE_MODEL: str = \"kwkty/vxnli-v0\"\n",
    "\n",
    "MAX_SOURCE_LENGTH: int = 1024\n",
    "MAX_TARGET_LENGTH: int = 124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TapexTokenizer.from_pretrained(\n",
    "    BASE_MODEL, use_fast=True, add_prefix_space=True\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"[arg]\", \"[kwarg]\", \"[eq]\"]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50268, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = BartConfig.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    no_repeat_ngram_size=0,\n",
    "    max_length=MAX_SOURCE_LENGTH,\n",
    "    early_stopping=False,\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    config=model_config,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_type_code</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>Apple</td>\n",
       "      <td>5.475398e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>jcrew</td>\n",
       "      <td>3.059093e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1.026885e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>Apple</td>\n",
       "      <td>2.295667e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>jcrew</td>\n",
       "      <td>5.927022e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id product_type_code product_name  product_price\n",
       "0           1          Hardware        Apple   5.475398e+07\n",
       "1           2           Clothes        jcrew   3.059093e+07\n",
       "2           3          Hardware        Apple   1.026885e+04\n",
       "3           4          Hardware        Apple   2.295667e+07\n",
       "4           5           Clothes        jcrew   5.927022e+06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_table(db_id: str, table_name: str) -> pd.DataFrame:\n",
    "    db_path = DATABASE_DIR.joinpath(f\"{db_id}/{db_id}.sqlite\")\n",
    "\n",
    "    with sqlite3.connect(db_path) as con:\n",
    "        return pd.read_sql(f\"SELECT * FROM {table_name}\", con)\n",
    "\n",
    "\n",
    "# Example\n",
    "load_table(\"customers_and_products_contacts\", \"products\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_type_code</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hardware</td>\n",
       "      <td>apple</td>\n",
       "      <td>54753982.574522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>clothes</td>\n",
       "      <td>jcrew</td>\n",
       "      <td>30590929.528306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>hardware</td>\n",
       "      <td>apple</td>\n",
       "      <td>10268.85297069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>hardware</td>\n",
       "      <td>apple</td>\n",
       "      <td>22956668.699482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>clothes</td>\n",
       "      <td>jcrew</td>\n",
       "      <td>5927021.8748021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id product_type_code product_name    product_price\n",
       "0          1          hardware        apple  54753982.574522\n",
       "1          2           clothes        jcrew  30590929.528306\n",
       "2          3          hardware        apple   10268.85297069\n",
       "3          4          hardware        apple  22956668.699482\n",
       "4          5           clothes        jcrew  5927021.8748021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns={col: col.lower() for col in df.columns})\n",
    "\n",
    "    # The TAPEX tokenizer raises an error when the table contains non-str columns\n",
    "    df = df.astype(str)\n",
    "\n",
    "    for col_name, col_dtype in zip(df.columns, df.dtypes):\n",
    "        df[col_name] = df[col_name].str.lower()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "preprocess_table(load_table(\"customers_and_products_contacts\", \"products\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functools.cache is supported in python3.9+, but use lru_cache to support python3.7+\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def load_and_preprocess_table(db_id: str, table_name: str) -> pd.DataFrame:\n",
    "    table = load_table(db_id, table_name)\n",
    "    table = preprocess_table(table)\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "    table = load_and_preprocess_table(example[\"db_id\"], example[\"table\"])\n",
    "\n",
    "    query = example[\"query\"]\n",
    "    answer = example[\"vega_zero\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        table=table,\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        answer=answer,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(*args, **kwargs) -> str:\n",
    "    args = (str(arg) for arg in args)\n",
    "    args = \" [arg] \".join(args)\n",
    "    args = f\"[arg] {args}\"\n",
    "\n",
    "    kwargs = (f\"{k} [eq] {v}\" for k, v in kwargs.items())\n",
    "    kwargs = \" [kwarg] \".join(kwargs)\n",
    "    kwargs = f\"[kwarg] {kwargs}\"\n",
    "\n",
    "    return f\"{args} {kwargs}\".lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vxnli_dataset(subset: str) -> Dataset:\n",
    "    # datasets.load_dataset(\"json\", PATH) raises an json parse error\n",
    "    # this is probably because it cannot parse the args and kwargs columns (list and dict types) well\n",
    "\n",
    "    df = pd.read_json(DATASET_DIR.joinpath(f\"{subset}.ndjson\"), lines=True)\n",
    "    df[\"query\"] = df.apply(\n",
    "        lambda row: preprocess_query(*row[\"args\"], **row[\"kwargs\"]), axis=1\n",
    "    )\n",
    "    df = df.drop(columns=[\"args\", \"kwargs\"])\n",
    "\n",
    "    return Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['db_id', 'table', 'chart', 'hardness', 'vega_zero', 'query', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['db_id', 'table', 'chart', 'hardness', 'vega_zero', 'query', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 270\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['db_id', 'table', 'chart', 'hardness', 'vega_zero', 'query', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET_OUTPUT_DIR.exists():\n",
    "    # load_from_dist doesn't support pathlib.Path\n",
    "    dataset = datasets.load_from_disk(str(DATASET_OUTPUT_DIR))\n",
    "else:\n",
    "    dataset = DatasetDict()\n",
    "\n",
    "    dataset[\"train\"] = load_vxnli_dataset(\"train\")\n",
    "    dataset[\"test\"] = load_vxnli_dataset(\"test\")\n",
    "    dataset[\"validation\"] = load_vxnli_dataset(\"val\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        preprocess_dataset,\n",
    "        batched=False,\n",
    "        num_proc=multiprocessing.cpu_count(),\n",
    "    )\n",
    "\n",
    "    # save_to_disk doesn't support pathlib.Path\n",
    "    dataset.save_to_disk(str(DATASET_OUTPUT_DIR))\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    preds = tokenizer.batch_decode(\n",
    "        preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    return exact_match.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/kwkty/vxnli-v1 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_OUTPUT_DIR,\n",
    "        predict_with_generate=True,\n",
    "        num_train_epochs=50,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        do_eval=True,\n",
    "        metric_for_best_model=\"exact_match\",\n",
    "        push_to_hub=push_model_to_huggingface_hub,\n",
    "        report_to=\"wandb\" if report_to_wandb else \"none\",\n",
    "    ),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=5),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/jupyter/vxnli/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1260\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7900\n",
      "  Number of trainable parameters = 139422720\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkwkty\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/vxnli/data/wandb/run-20221225_042742-3grk8w92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kwkty/vxnli/runs/3grk8w92\" target=\"_blank\">../data/models/vxnli-v1</a></strong> to <a href=\"https://wandb.ai/kwkty/vxnli\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1896' max='7900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1896/7900 20:19 < 1:04:27, 1.55 it/s, Epoch 12/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.299030</td>\n",
       "      <td>0.274074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.219206</td>\n",
       "      <td>0.525926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.234584</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.223286</td>\n",
       "      <td>0.588889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.219652</td>\n",
       "      <td>0.562963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.244793</td>\n",
       "      <td>0.574074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.246876</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.254745</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.249572</td>\n",
       "      <td>0.581481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.293903</td>\n",
       "      <td>0.559259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.287867</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.258056</td>\n",
       "      <td>0.581481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-158\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-158/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-158/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-158/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-158/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-158/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-316\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-316/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-316/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-316/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-158] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-474\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-474/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-474/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-474/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-474/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-474/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-316] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-632\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-632/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-632/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-632/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-632/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-632/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-474] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-790\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-790/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-790/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-790/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-790/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-790/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-948\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-948/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-948/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-948/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-948/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-948/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-790] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1106\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1106/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1106/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1106/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1106/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1106/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-632] due to args.save_total_limit\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-948] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1264\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1264/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1264/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1264/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1264/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1264/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1422\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1422/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1422/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1422/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1422/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1422/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-1264] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1580\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1580/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1580/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1580/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1580/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1580/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-1422] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1738\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1738/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1738/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1738/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1738/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1738/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-1580] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../data/models/vxnli-v1/checkpoint-1896\n",
      "Configuration saved in ../data/models/vxnli-v1/checkpoint-1896/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/checkpoint-1896/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/checkpoint-1896/added_tokens.json\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-1738] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../data/models/vxnli-v1/checkpoint-1106 (score: 0.6111111111111112).\n",
      "Deleting older checkpoint [../data/models/vxnli-v1/checkpoint-1896] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1896, training_loss=0.10589330501948731, metrics={'train_runtime': 1223.285, 'train_samples_per_second': 51.501, 'train_steps_per_second': 6.458, 'total_flos': 8911117321666560.0, 'train_loss': 0.10589330501948731, 'epoch': 12.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25023138523101807,\n",
       " 'eval_exact_match': 0.5444444444444444,\n",
       " 'eval_runtime': 24.8449,\n",
       " 'eval_samples_per_second': 10.867,\n",
       " 'eval_steps_per_second': 1.368,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.evaluate must be called for the model card\n",
    "\n",
    "trainer.evaluate(dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ds: Dataset) -> List[str]:\n",
    "    preds = trainer.predict(\n",
    "        ds,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "    )\n",
    "\n",
    "    preds = tokenizer.batch_decode(\n",
    "        preds.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    return [pred.strip() for pred in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: table, db_id, chart, vega_zero, query, hardness. If table, db_id, chart, vega_zero, query, hardness are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark point encoding x investor_id y aggregate mean share_count transform group x',\n",
       "  'mark point encoding x investor_id y aggregate mean share_count transform group x'],\n",
       " ['mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark bar encoding x name y aggregate none weight transform sort x asc',\n",
       "  'mark point encoding x investor_id y aggregate mean share_count transform group x',\n",
       "  'mark point encoding x investor_id y aggregate mean share_count transform group x'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(dataset[\"test\"])\n",
    "\n",
    "preds[:5], dataset[\"test\"][\"vega_zero\"][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.5407407407407407}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match.compute(\n",
    "    predictions=preds,\n",
    "    references=dataset[\"test\"][\"vega_zero\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>table</th>\n",
       "      <th>chart</th>\n",
       "      <th>hardness</th>\n",
       "      <th>vega_zero</th>\n",
       "      <th>query</th>\n",
       "      <th>pred</th>\n",
       "      <th>exact_matched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_poll</td>\n",
       "      <td>people</td>\n",
       "      <td>bar</td>\n",
       "      <td>Easy</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>[arg]  [kwarg] use_bar_chart [eq] true [kwarg]...</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_poll</td>\n",
       "      <td>people</td>\n",
       "      <td>bar</td>\n",
       "      <td>Easy</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>[arg] use a bar chart [kwarg] x [eq] name [kwa...</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_poll</td>\n",
       "      <td>people</td>\n",
       "      <td>bar</td>\n",
       "      <td>Easy</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>[arg]  [kwarg] graph [eq] bar [kwarg] x [eq] n...</td>\n",
       "      <td>mark bar encoding x name y aggregate none weig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tracking_share_transactions</td>\n",
       "      <td>transactions</td>\n",
       "      <td>point</td>\n",
       "      <td>Easy</td>\n",
       "      <td>mark point encoding x investor_id y aggregate ...</td>\n",
       "      <td>[arg] scatter chart [arg] investor id and mean...</td>\n",
       "      <td>mark point encoding x investor_id y aggregate ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tracking_share_transactions</td>\n",
       "      <td>transactions</td>\n",
       "      <td>point</td>\n",
       "      <td>Easy</td>\n",
       "      <td>mark point encoding x investor_id y aggregate ...</td>\n",
       "      <td>[arg]  [kwarg] graph_type [eq] scatter [kwarg]...</td>\n",
       "      <td>mark point encoding x investor_id y aggregate ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>tracking_share_transactions</td>\n",
       "      <td>transactions</td>\n",
       "      <td>Line</td>\n",
       "      <td>Medium</td>\n",
       "      <td>mark line encoding x date_of_transaction y agg...</td>\n",
       "      <td>[arg]  [kwarg] time_axis [eq] date_of_transact...</td>\n",
       "      <td>mark line encoding x date_of_transaction y agg...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>tracking_share_transactions</td>\n",
       "      <td>transactions</td>\n",
       "      <td>Line</td>\n",
       "      <td>Medium</td>\n",
       "      <td>mark line encoding x date_of_transaction y agg...</td>\n",
       "      <td>[arg] show me a trend [kwarg] x [eq] date_of_t...</td>\n",
       "      <td>mark line encoding x date_of_transaction y agg...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>customers_and_invoices</td>\n",
       "      <td>financial_transactions</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Medium</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>[arg] show the transaction types and the total...</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>customers_and_invoices</td>\n",
       "      <td>financial_transactions</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Medium</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>[arg]  [kwarg] x [eq] type [kwarg] y [eq] amou...</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>customers_and_invoices</td>\n",
       "      <td>financial_transactions</td>\n",
       "      <td>Bar</td>\n",
       "      <td>Medium</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>[arg] ['transaction_type', 'sum(transaction_am...</td>\n",
       "      <td>mark bar encoding x transaction_type y aggrega...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           db_id                   table  chart hardness  \\\n",
       "0                 candidate_poll                  people    bar     Easy   \n",
       "1                 candidate_poll                  people    bar     Easy   \n",
       "2                 candidate_poll                  people    bar     Easy   \n",
       "3    tracking_share_transactions            transactions  point     Easy   \n",
       "4    tracking_share_transactions            transactions  point     Easy   \n",
       "..                           ...                     ...    ...      ...   \n",
       "265  tracking_share_transactions            transactions   Line   Medium   \n",
       "266  tracking_share_transactions            transactions   Line   Medium   \n",
       "267       customers_and_invoices  financial_transactions    Bar   Medium   \n",
       "268       customers_and_invoices  financial_transactions    Bar   Medium   \n",
       "269       customers_and_invoices  financial_transactions    Bar   Medium   \n",
       "\n",
       "                                             vega_zero  \\\n",
       "0    mark bar encoding x name y aggregate none weig...   \n",
       "1    mark bar encoding x name y aggregate none weig...   \n",
       "2    mark bar encoding x name y aggregate none weig...   \n",
       "3    mark point encoding x investor_id y aggregate ...   \n",
       "4    mark point encoding x investor_id y aggregate ...   \n",
       "..                                                 ...   \n",
       "265  mark line encoding x date_of_transaction y agg...   \n",
       "266  mark line encoding x date_of_transaction y agg...   \n",
       "267  mark bar encoding x transaction_type y aggrega...   \n",
       "268  mark bar encoding x transaction_type y aggrega...   \n",
       "269  mark bar encoding x transaction_type y aggrega...   \n",
       "\n",
       "                                                 query  \\\n",
       "0    [arg]  [kwarg] use_bar_chart [eq] true [kwarg]...   \n",
       "1    [arg] use a bar chart [kwarg] x [eq] name [kwa...   \n",
       "2    [arg]  [kwarg] graph [eq] bar [kwarg] x [eq] n...   \n",
       "3    [arg] scatter chart [arg] investor id and mean...   \n",
       "4    [arg]  [kwarg] graph_type [eq] scatter [kwarg]...   \n",
       "..                                                 ...   \n",
       "265  [arg]  [kwarg] time_axis [eq] date_of_transact...   \n",
       "266  [arg] show me a trend [kwarg] x [eq] date_of_t...   \n",
       "267  [arg] show the transaction types and the total...   \n",
       "268  [arg]  [kwarg] x [eq] type [kwarg] y [eq] amou...   \n",
       "269  [arg] ['transaction_type', 'sum(transaction_am...   \n",
       "\n",
       "                                                  pred  exact_matched  \n",
       "0    mark bar encoding x name y aggregate none weig...           True  \n",
       "1    mark bar encoding x name y aggregate none weig...           True  \n",
       "2    mark bar encoding x name y aggregate none weig...           True  \n",
       "3    mark point encoding x investor_id y aggregate ...           True  \n",
       "4    mark point encoding x investor_id y aggregate ...           True  \n",
       "..                                                 ...            ...  \n",
       "265  mark line encoding x date_of_transaction y agg...          False  \n",
       "266  mark line encoding x date_of_transaction y agg...          False  \n",
       "267  mark bar encoding x transaction_type y aggrega...          False  \n",
       "268  mark bar encoding x transaction_type y aggrega...          False  \n",
       "269  mark bar encoding x transaction_type y aggrega...          False  \n",
       "\n",
       "[270 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df = dataset[\"test\"].to_pandas()\n",
    "preds_df = preds_df.drop(columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "preds_df[\"pred\"] = preds\n",
    "preds_df[\"exact_matched\"] = preds_df[\"pred\"] == preds_df[\"vega_zero\"]\n",
    "\n",
    "preds_df.to_csv(RESULT_OUTPUT_DIR.joinpath(\"prediction.csv\"))\n",
    "\n",
    "preds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Easy</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Hard</th>\n",
       "      <th>Extra Hard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>52</td>\n",
       "      <td>80</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Easy  Medium  Hard  Extra Hard\n",
       "True     52      80    13           1\n",
       "False    29      52    26          17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        preds_df[preds_df[\"hardness\"] == hardness][\"exact_matched\"]\n",
    "        .value_counts()\n",
    "        .rename(hardness)\n",
    "        for hardness in (\"Easy\", \"Medium\", \"Hard\", \"Extra Hard\")\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>point</th>\n",
       "      <th>arc</th>\n",
       "      <th>line</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Stacked Bar</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>96</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>63</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bar  point  arc  line  Bar  Stacked Bar  Line\n",
       "True    96     12   12    11   12            2     1\n",
       "False   63     18    3    10   24            1     5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        preds_df[preds_df[\"chart\"] == chart][\"exact_matched\"]\n",
    "        .value_counts()\n",
    "        .rename(chart)\n",
    "        for chart in preds_df[\"chart\"].unique()\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622a9c45e5c74aeeac2022accc564636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match</td><td>0.54444</td></tr><tr><td>eval/loss</td><td>0.25023</td></tr><tr><td>eval/runtime</td><td>24.8449</td></tr><tr><td>eval/samples_per_second</td><td>10.867</td></tr><tr><td>eval/steps_per_second</td><td>1.368</td></tr><tr><td>train/epoch</td><td>12.0</td></tr><tr><td>train/global_step</td><td>1896</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0096</td></tr><tr><td>train/total_flos</td><td>8911117321666560.0</td></tr><tr><td>train/train_loss</td><td>0.10589</td></tr><tr><td>train/train_runtime</td><td>1223.285</td></tr><tr><td>train/train_samples_per_second</td><td>51.501</td></tr><tr><td>train/train_steps_per_second</td><td>6.458</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">../data/models/vxnli-v1</strong>: <a href=\"https://wandb.ai/kwkty/vxnli/runs/3grk8w92\" target=\"_blank\">https://wandb.ai/kwkty/vxnli/runs/3grk8w92</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/jupyter/vxnli/data/wandb/run-20221225_042742-3grk8w92/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if report_to_wandb:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../data/models/vxnli-v1\n",
      "Configuration saved in ../data/models/vxnli-v1/config.json\n",
      "Model weights saved in ../data/models/vxnli-v1/pytorch_model.bin\n",
      "tokenizer config file saved in ../data/models/vxnli-v1/tokenizer_config.json\n",
      "Special tokens file saved in ../data/models/vxnli-v1/special_tokens_map.json\n",
      "added tokens file saved in ../data/models/vxnli-v1/added_tokens.json\n",
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/kwkty/vxnli-v1\n",
      "   609b05e..94c14ff  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "To https://huggingface.co/kwkty/vxnli-v1\n",
      "   94c14ff..1c5562d  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if push_model_to_huggingface_hub:\n",
    "    trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63d5e78cacc7bbb3aa8f0f1cd8b8015c0d580cb8c6884f2092d527cf506d691b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
